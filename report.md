# Практическое задание 
- Дисциплина: *"Параллельное программирование на суперкомпьютерных системах"*
- Тема: *Алгоритм имитации отжига методом Монте Карло*
- Прикладная задача: Квадратичная задача о назначениях (Quadratic assignment problem)

# Введение
Цель работы - изучить особенности параллельной реализации вычислительных алгоритмов на разных технологиях параллелизма и сравнить их эффективность. Для выбранной вычислительной задачи требуется:
1. Выбрать прикладную задачу решаемую методом имитации отжига Монте-Карло.
2. Предложить алгоритм решения и рассмотреть возможные подходы распараллеивания. 
3. Провести ряд экспериментом и проанализировать полученные результаты.
4. Реализовать алгоритм четырьмя способами:

 - C + pthreads (Linux Native Threads);
 - C + MPI;
 - Python + MPI (mpi4py)
 - C + OpenMP

# Прикладная задача

Квадратичная задача о назначениях (Quadratic Assignment Problem, QAP) - классическая NP-полная комбинаторная оптимизационная задача.

Формулировка:
Дано два квадратных матрицы размера $ n \times n $:

* матрица потоков $ F = [f_{ij}] $, где $ f_{ij} $ - интенсивность взаимодействия между объектами $i$ и $j$;
* матрица расстояний $D = [d_{kl}]$, где $d_{kl}$ — расстояние между позициями $k$ и $l$.

Требуется найти перестановку
$$
\pi = {\pi(1), \pi(2), \dots, \pi(n)},
$$
которая минимизирует суммарную стоимость размещения объектов:

$$
\min_{\pi}  C(\pi) =
\sum_{i=1}^{n}\sum_{j=1}^{n} f_{ij} \cdot d_{\pi(i),\pi(j)}.
$$
---

## Интерпретация задачи

Классический пример QAP — размещение цехов завода в заданных локациях.

* $f_{ij} $ — поток материалов между цехами $i$ и $j$;
* $ d_{kl} $ — расстояние между площадками $k$ и $l$;
* Нужно сопоставить каждый цех конкретной площадке так, чтобы минимизировать суммарные логистические затраты.

---

## Пример

Пусть $n = 3$.
Матрица потоков:

$$
F =
\begin{pmatrix}
0 & 5 & 2 \\
5 & 0 & 3 \\
2 & 3 & 0
\end{pmatrix}
$$

Матрица расстояний:

$$
D =
\begin{pmatrix}
0 & 4 & 1 \\
4 & 0 & 2 \\
1 & 2 & 0
\end{pmatrix}
$$

Если выбрать перестановку ( $\pi = (2,3,1)$ ), то стоимость:

$$
C(\pi) =
\sum_{i=1}^{3}\sum_{j=1}^{3}
f_{ij} \cdot d_{\pi(i),\pi(j)}.
$$

После подстановки вычисляется итоговое значение стоимости размещения.


# Алгоритм имитации отжига Монте-Карло

1. Дано: матрицы $F, D$; начальная температура $T$; начальная перестановка $\pi$; число итераций $K$.
2. Выход: стоимость пути $\pi_{best}$.

```
Для каждой итерации:
      a) Сгенерировать соседнее решение π' 
         (обмен двух случайных позиций).
      b) Вычислить Δ = C(π') − C(π).
      c) Если Δ ≤ 0, принять новую перестановку.
      d) Иначе принять с вероятностью exp(−Δ / T).
      e) Понизить температуру согласно T = T * α.
Вернуть найденное минимальное решение.
```

# Датасет
Рассматриваются наборы данных с сайта [qaplib](https://qaplib.mgi.polymtl.ca/), для представленных данных заданы матрицы потоков и расстояний. Приведены оптимальные решения для сравнения найденного решения.

# C + pthreads

В основном потоке создаем $N$ рабочих потоков (pthreads), каждый из которых получается независимые данные одинаковые входные данные. Это предотвращает необходимость синхронизации и исключает накладные расходы на блокировки. Каждый поток выполняет собственный независимый метод имитации отжига. После выполнения $K$ итераций производится поиск лучшего решения. 

# C + MPI / Python + MPI
Принцип параллелизма используется такой же как и в `C+pthreads`, особенностью реализации является передача данных с помощью `MPI_Bcast` также каждые 1000 итераций поток 0 передает лучшую перестановку другим процессам с некоторой вероятностью, чтобы ускорить процесс вычислений.


# C + OpenMP
Каждый поток OpenMP независимо выполняет полный цикл имитации отжига со своим и выбирается глобально лучшее решение. (Симметрично pthreads)

# Анализ результатов
Были проведены и поставлены эксперименты на входных данных `tai256c.dat` ($N=256$).


Число процессов в многопоточном режиме: от 2 до 56.
## Single thread
В однопоточном режиме получены следующие результаты:

<img src="figs/single_thread.png" width="600">

Среднее время когда решение замораживается: 0.4466 сек.
Отклонение от лучшего результата 2.1%.
## C+pthreads
Для pthreads получены следующие результаты:

<img src="figs/pthreads.png" width="600">

| Число потоков  | Время достижения оптимума (сек)  | "Оптимум"       |
| -------------- | -------------------------------- | --------------- |
| 6              | 0.396968                         | 44885078 (1.8%) |
| 12             | 0.315088                         | 44876026 (1.78%)   |       
| 24             | 0.224453                         | 44865058 (1.77%)    |
| 48             | 0.211214                         | 44862452 (1.74%)    |
| 56             | 0.207943                         | 44853162 (1.71%)    |

В многопоточном режиме алгоритм позволяет найти более лучшее решение за более лучшее время.

## MPI
### MPI + C

Для `MPI+C` получены следующие результаты:

### 1 Узел

<img src="figs/mpi_c.png" width="600">

| Число потоков  | Время достижения оптимума (сек)  | "Оптимум"       |
| -------------- | -------------------------------- | --------------- |
| 6              | 0.465376                         | 44991922 (2.03%) |
| 12             | 0.345530                         | 44981368 (2.01%)   |       
| 24             | 0.283858                         | 44958836 (1.98%)    |
| 48             | 0.271214                         | 44958836 (1.98%)    |
| 56             | 0.247943                         | 44958836 (1.98%)    |


### 2 Узла

<img src="figs/mpi_c2.png" width="600">

| Число потоков  | Время достижения оптимума (сек)  | "Оптимум"       |
| -------------- | -------------------------------- | --------------- |
| 6              | 0.318466                         | 44958836 (1.98%)  |
| 12             | 0.307006                         | 44958836 (1.98%)  |       
| 24             | 0.319122                         | 44958836 (1.98%)    |
| 48             | 0.304641                         | 44958836 (1.98%)    |
| 56             | 0.327066                         | 44958836 (1.98%)    |


При обмене "лучшим" вариантом возникает место синхронизации потоков, за счет чего и снижается производительность алгоритма.

### MPI + Python
Для `MPI+Python` получены следующие результаты:

Для Python версии запуски производились на датасете меньшей размерности (n=128)

<img src="figs/mpi_py.png" width="600">

| Число потоков | Время достижения оптимума (сек) |
| ------------- | ------------------------------- |
| 1             | 23.594121                       |
| 2             | 23.111854                       |
| 3             | 27.796782                       |
| 6             | 27.889543                       |
| 9             | 29.58954                        |
| 12            | 29.69403                        |


График сравнения 1, 2 и 12 потоков:

<img src="figs/mpi_py_1_2_12.png" width="600">

Python версия демонстрирует слабое масштабирование, поскольку вычислительные затраты и высокая стоимость межпроцессных коммуникаций полностью перекрывают потенциальный выигрыш от параллелизма.


### OpenMP

Для `C+OpenMP` получены следующие результаты:

<img src="figs/openmp.png" width="600">

| Число потоков  | Время достижения оптимума (сек)  | "Оптимум"       |
| -------------- | -------------------------------- | --------------- |
| 6              | 0.190702                         | 44950306 (1.93%)  |
| 12             | 0.188951                        | 44950306 (1.93%)  |       
| 24             | 0.188712                         | 44950306 (1.93%)    |
| 48             | 0.247208                         | 44950306 (1.93%)    |
| 56             | 0.366531                         | 44950306 (1.93%)    |


OpenMP показывает ускорение только на малом числе потоков, а при дальнейшем увеличении производительности падает из-за роста накладных расходов и конкуренции потоков за общие вычислительные ресурсы.


## Сравнение результатов
| Реализация    | Платформа | Узлы | Макс. параллелизм | Лучшее время до «заморозки», сек | Отклонение от оптимума |
| ------------- | --------- | ---- | ----------------- | -------------------------------- | ---------------------- |
| Single thread | C         | 1    | 1                 | 0.4466                           | 2.1%                      |
| C + pthreads  | C         | 1    | 56 потоков        | **0.2079**                       | **1.71 %**             |
| C + MPI       | C         | 1    | 56 процессов      | 0.2479                           | 1.98 %                 |
| C + MPI       | C         | 2    | 56 процессов      | 0.3046                           | 1.98 %                 |
| MPI + Python  | Python    | 1    | 12 процессов      | 23.11                            | —                      |
| C + OpenMP    | C         | 1    | 24 потока         | **0.1887**                       | 1.93 %                 |


Наилучшую производительность даёт pthreads-реализация  (используется независимая параллеизация без обмена между потоками). MPI и OpenMP эффективно масштабируются только при небольшом числе процессов из-за накладных расходов.

# Заключение
В работе были исследованы различные подходы к параллельной реализации алгоритма имитации отжига для решения квадратичной задачи о назначениях, с анализом их производительности и масштабируемости на современных многопроцессорных архитектурах.


Были реализованы две различные схемы параллелизма: с обменом между потоками и без. По проведенным численным экспериментам независимая схема показывает более эффективный результат.

